{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from typing import Dict\n",
    "import json\n",
    "from IPython.display import display\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "from parlai_internal.projects.light.lightqa.results.utils import get_eval_directories, df_from_eval_runs, save_to_csv, load_all_exps, get_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_paths = {\n",
    "    # WizWiki\n",
    "    'wow_baseline': '/checkpoint/light/projects/lightqa/wow/evals/wow_baseline',\n",
    "    'wow_k2r': '/checkpoint/light/projects/lightqa/wow/evals/wow_k2r',\n",
    "    \n",
    "    # NQ\n",
    "    'nq_k2r': '/checkpoint/light/projects/lightqa/nq_open/evals/nq_k2r',\n",
    "    'nq_baseline': '/checkpoint/light/projects/lightqa/nq_open/evals/nq_baseline',\n",
    "    \n",
    "    # LIGHT\n",
    "    'lightqa_baseline': '/checkpoint/light/projects/lightqa/lightqa/evals/lightqa_baseline',\n",
    "    'lightqa_k2r': '/checkpoint/light/projects/lightqa/lightqa/evals/lightqa_k2r',\n",
    "    \n",
    "    #WizInt\n",
    "    'woi_k2r': '/checkpoint/light/projects/lightqa/woi/evals/woi_fidgold',\n",
    "}\n",
    "\n",
    "dfs, df = load_all_exps(eval_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightWild test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Quantitative Evaluations on LightWild test\n",
    "display(get_view(\n",
    "    dfs['lightqa_baseline'],\n",
    "    conditions={\n",
    "        'dt': 'test',\n",
    "        't': 'LightTeacherPlus'\n",
    "    }))\n",
    "print('Table: Baseline LightWild test')\n",
    "\n",
    "\n",
    "df_view = get_view(\n",
    "    dfs['lightqa_k2r'],\n",
    "    conditions={\n",
    "        'dt': 'test',\n",
    "        't': 'LightTeacherPlus',\n",
    "    })\n",
    "display(df_view[df_view['response-model'] != 'BART conf-score'])\n",
    "print('Table: K2R LightWild test')\n",
    "\n",
    "\n",
    "df_view = get_view(\n",
    "    dfs['lightqa_k2r'],\n",
    "    conditions={\n",
    "        'dt': 'test',\n",
    "        't': 'LightTeacherPlus',\n",
    "        'response-model': 'BART conf-score',\n",
    "    }, sort_key='ppl')\n",
    "display(df_view[(df_view['km-train-data'] == 'Light') | (df_view['km-train-data'] == '-')])\n",
    "print('Table: Confidence-Score Exps LightWild test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightWild valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative Evaluations on LightWild valid\n",
    "display(get_view(\n",
    "    dfs['lightqa_baseline'],\n",
    "    conditions={\n",
    "        'dt': 'valid',\n",
    "        't': 'LightTeacherPlus'\n",
    "    }))\n",
    "print('Table: Baseline LightWild valid')\n",
    "\n",
    "df_view = get_view(\n",
    "    dfs['lightqa_k2r'],\n",
    "    conditions={\n",
    "        'dt': 'valid',\n",
    "        't': 'LightTeacherPlus',\n",
    "    })\n",
    "display(df_view[df_view['response-model'] != 'BART conf-score'])\n",
    "print('Table: K2R LightWild valid')\n",
    "\n",
    "\n",
    "df_view = get_view(\n",
    "    dfs['lightqa_k2r'],\n",
    "    conditions={\n",
    "        'dt': 'valid',\n",
    "        't': 'LightTeacherPlus',\n",
    "        'response-model': 'BART conf-score',\n",
    "    }, sort_key='ppl')\n",
    "display(df_view[(df_view['km-train-data'] == 'Light') | (df_view['km-train-data'] == '-')])\n",
    "print('Table: Confidence-Score Exps LightWild valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightQA test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative Evaluations on LightQA test\n",
    "display(get_view(\n",
    "    dfs['lightqa_baseline'],\n",
    "    conditions={\n",
    "        'dt': 'test',\n",
    "        't': 'SummaryQATeacher'\n",
    "    }))\n",
    "print('Table: Baseline LightQA test')\n",
    "\n",
    "df_view = get_view(\n",
    "    dfs['lightqa_k2r'],\n",
    "    conditions={\n",
    "        'dt': 'test',\n",
    "        't': 'SummaryQATeacher',\n",
    "    })\n",
    "display(df_view[df_view['response-model'] != 'BART conf-score'])\n",
    "print('Table: K2R LightQA test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightQA valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative Evaluations on LightQA valid\n",
    "display(get_view(\n",
    "    dfs['lightqa_baseline'],\n",
    "    conditions={\n",
    "        'dt': 'valid',\n",
    "        't': 'SummaryQATeacher'\n",
    "    }))\n",
    "print('Table: Baseline LightQA valid')\n",
    "\n",
    "\n",
    "df_view = get_view(\n",
    "    dfs['lightqa_k2r'],\n",
    "    conditions={\n",
    "        'dt': 'valid',\n",
    "        't': 'SummaryQATeacher',\n",
    "    })\n",
    "display(df_view[df_view['response-model'] != 'BART conf-score'])\n",
    "print('Table: K2R LightQA valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NQ Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NQ test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative Evaluations on NQ test\n",
    "display(get_view(\n",
    "    dfs['nq_baseline'],\n",
    "    conditions={\n",
    "        'dt': 'test',\n",
    "        'beam-size': '3',\n",
    "    }))\n",
    "print('Table: Baseline NQ test')\n",
    "\n",
    "df_view = get_view(\n",
    "    dfs['nq_k2r'],\n",
    "    conditions={\n",
    "        'dt': 'test',\n",
    "    })\n",
    "df_view = df_view[df_view['knowledge-model'] != 'Oracle']\n",
    "display(df_view)\n",
    "print('Table: K2R NQ test')\n",
    "\n",
    "df_oracle = get_view(\n",
    "    dfs['nq_k2r'],\n",
    "    conditions={\n",
    "        'dt': 'test',\n",
    "        'knowledge-model': 'Oracle',\n",
    "    })\n",
    "display(df_oracle)\n",
    "print('Table: Oracle K2R NQ test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NQ valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative Evaluations on NQ valid\n",
    "display(get_view(\n",
    "    dfs['nq_baseline'],\n",
    "    conditions={\n",
    "        'dt': 'valid',\n",
    "        'beam-size': '3',\n",
    "    }))\n",
    "print('Table: Baseline NQ valid')\n",
    "\n",
    "df_view = get_view(\n",
    "    dfs['nq_k2r'],\n",
    "    conditions={\n",
    "        'dt': 'valid',\n",
    "    })\n",
    "df_view = df_view[df_view['knowledge-model'] != 'Oracle']\n",
    "display(df_view)\n",
    "print('Table: K2R NQ valid')\n",
    "\n",
    "df_oracle = get_view(\n",
    "    dfs['nq_k2r'],\n",
    "    conditions={\n",
    "        'dt': 'valid',\n",
    "        'knowledge-model': 'Oracle',\n",
    "    })\n",
    "display(df_oracle)\n",
    "print('Table: Oracle K2R NQ valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WizWiki Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WizWiki test seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Quantitative Evaluations on WizWiki test seen\n",
    "df_view = get_view(\n",
    "    dfs['wow_baseline'],\n",
    "    conditions={\n",
    "        'datatype': 'test',\n",
    "        't': 'random_split',\n",
    "    })\n",
    "\n",
    "display(df_view)\n",
    "print('Table: Baseline WizWiki test seen')\n",
    "\n",
    "df_view = get_view(\n",
    "    dfs['wow_k2r'],\n",
    "    conditions={\n",
    "        'dt': 'test',\n",
    "        't': 'WoW seen',\n",
    "    })\n",
    "df_view = df_view[df_view['response-model'] != 'BART conf-score']\n",
    "df_view = df_view[df_view['knowledge-model'] != 'Oracle']\n",
    "display(df_view)\n",
    "print('Table: K2R WizWiki test seen')\n",
    "\n",
    "df_view = get_view(\n",
    "    dfs['wow_k2r'],\n",
    "    conditions={\n",
    "        'dt': 'test',\n",
    "        't': 'WoW seen',\n",
    "    })\n",
    "df_view = df_view[df_view['response-model'] != 'BART conf-score']\n",
    "df_view = df_view[df_view['knowledge-model'] == 'Oracle']\n",
    "display(df_view)\n",
    "print('Table: K2R-Oracle WizWiki test seen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WizWiki confidence-score conditioned Exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_view = get_view(\n",
    "    dfs['wow_k2r'],\n",
    "    conditions={\n",
    "        'dt': 'valid',\n",
    "        't': 'WoW seen',\n",
    "        'krm-beam-min-length': '10',\n",
    "    })\n",
    "df_view = df_view[df_view['response-model'] == 'BART conf-score']\n",
    "df_view = df_view[df_view['knowledge-model'] != 'Oracle']\n",
    "df_view['add-fixed-confidence'] = df_view['add-fixed-confidence'].astype(int)\n",
    "df_view.sort_values('add-fixed-confidence', inplace=True)\n",
    "display(df_view)\n",
    "print('Table: K2R confidence-conditioned WizWiki test seen')\n",
    "\n",
    "df_view = get_view(\n",
    "    dfs['wow_k2r'],\n",
    "    conditions={\n",
    "        'dt': 'valid',\n",
    "        't': 'WoW seen',\n",
    "        'krm-beam-min-length': '10',\n",
    "    })\n",
    "df_view = df_view[df_view['response-model'] == 'BART conf-score']\n",
    "df_view = df_view[df_view['knowledge-model'] == 'Oracle']\n",
    "df_view['add-fixed-confidence'] = df_view['add-fixed-confidence'].astype(int)\n",
    "df_view.sort_values('add-fixed-confidence', inplace=True)\n",
    "display(df_view)\n",
    "print('Table: K2R-Oracle confidence-conditioned WizWiki test seen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WizWiki test unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative Evaluations on WizWiki test unseen\n",
    "df_view = get_view(\n",
    "    dfs['wow_baseline'],\n",
    "    conditions={\n",
    "        'datatype': 'test',\n",
    "        't': 'topic_split',\n",
    "    })\n",
    "\n",
    "display(df_view)\n",
    "print('Table: Baseline WizWiki test unseen')\n",
    "\n",
    "df_view = get_view(\n",
    "    dfs['wow_k2r'],\n",
    "    conditions={\n",
    "        'dt': 'test',\n",
    "        't': 'WoW unseen',\n",
    "    })\n",
    "df_view = df_view[df_view['response-model'] != 'BART conf-score']\n",
    "df_view = df_view[df_view['knowledge-model'] != 'Oracle']\n",
    "display(df_view)\n",
    "print('Table: K2R WizWiki test unseen')\n",
    "\n",
    "df_view = get_view(\n",
    "    dfs['wow_k2r'],\n",
    "    conditions={\n",
    "        'dt': 'test',\n",
    "        't': 'WoW unseen',\n",
    "    })\n",
    "df_view = df_view[df_view['response-model'] != 'BART conf-score']\n",
    "df_view = df_view[df_view['knowledge-model'] == 'Oracle']\n",
    "display(df_view)\n",
    "print('Table: K2R-Oracle WizWiki test unseen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WizWiki valid seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative Evaluations on WizWiki valid seen\n",
    "df_view = get_view(\n",
    "    dfs['wow_baseline'],\n",
    "    conditions={\n",
    "        'datatype': 'valid',\n",
    "        't': 'random_split',\n",
    "    })\n",
    "\n",
    "display(df_view)\n",
    "print('Table: Baseline WizWiki valid seen')\n",
    "\n",
    "df_view = get_view(\n",
    "    dfs['wow_k2r'],\n",
    "    conditions={\n",
    "        'dt': 'valid',\n",
    "        't': 'WoW seen',\n",
    "    })\n",
    "df_view = df_view[df_view['response-model'] != 'BART conf-score']\n",
    "df_view = df_view[df_view['knowledge-model'] != 'Oracle']\n",
    "display(df_view)\n",
    "print('Table: K2R WizWiki valid seen')\n",
    "\n",
    "df_view = get_view(\n",
    "    dfs['wow_k2r'],\n",
    "    conditions={\n",
    "        'dt': 'valid',\n",
    "        't': 'WoW seen',\n",
    "    })\n",
    "df_view = df_view[df_view['response-model'] != 'BART conf-score']\n",
    "df_view = df_view[df_view['knowledge-model'] == 'Oracle']\n",
    "display(df_view)\n",
    "print('Table: K2R-Oracle WizWiki valid seen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WizWiki valid unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative Evaluations on WizWiki valid unseen\n",
    "df_view = get_view(\n",
    "    dfs['wow_baseline'],\n",
    "    conditions={\n",
    "        'datatype': 'valid',\n",
    "        't': 'topic_split',\n",
    "    })\n",
    "\n",
    "display(df_view)\n",
    "print('Table: Baseline WizWiki valid unseen')\n",
    "\n",
    "df_view = get_view(\n",
    "    dfs['wow_k2r'],\n",
    "    conditions={\n",
    "        'dt': 'valid',\n",
    "        't': 'WoW unseen',\n",
    "    })\n",
    "df_view = df_view[df_view['response-model'] != 'BART conf-score']\n",
    "df_view = df_view[df_view['knowledge-model'] != 'Oracle']\n",
    "display(df_view)\n",
    "print('Table: K2R WizWiki valid unseen')\n",
    "\n",
    "df_view = get_view(\n",
    "    dfs['wow_k2r'],\n",
    "    conditions={\n",
    "        'dt': 'valid',\n",
    "        't': 'WoW unseen',\n",
    "    })\n",
    "df_view = df_view[df_view['response-model'] != 'BART conf-score']\n",
    "df_view = df_view[df_view['knowledge-model'] == 'Oracle']\n",
    "display(df_view)\n",
    "print('Table: K2R-Oracle WizWiki valid unseen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wizard of Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(get_view(\n",
    "    dfs['woi_k2r'],\n",
    "    conditions={\n",
    "        'dt': 'valid',\n",
    "        'krm-beam-min-length': '10',\n",
    "    },\n",
    "    removed_cols = ['rm-train-data', 'km-train-data', 'complete_percentage']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_parlai",
   "language": "python",
   "name": "conda_parlai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
